{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras_preprocessing.image import load_img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_faces.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                        filename  age  gender  ethnicity\n0  100_1_0_20170110183726390.jpg  100       1          0\n1  100_1_2_20170105174847679.jpg  100       1          2\n2  100_1_2_20170110182836729.jpg  100       1          2\n3  101_1_2_20170105174739309.jpg  101       1          2\n4   10_0_0_20161220222308131.jpg   10       0          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>ethnicity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100_1_0_20170110183726390.jpg</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100_1_2_20170105174847679.jpg</td>\n      <td>100</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100_1_2_20170110182836729.jpg</td>\n      <td>100</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101_1_2_20170105174739309.jpg</td>\n      <td>101</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10_0_0_20161220222308131.jpg</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# map labels\n",
    "gender_dict = {0: 'Male', 1: 'Female'}\n",
    "ethnicity_dict = {}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d7074939e815>:8: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  img = img.resize((128, 128), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "features = []\n",
    "\n",
    "for image in df['filename']:\n",
    "    path = 'pic/' + image\n",
    "    img = load_img(path)\n",
    "    img = img.resize((128, 128), Image.ANTIALIAS)\n",
    "    img = np.array(img)\n",
    "    features.append(img)\n",
    "\n",
    "# Convert to np array, so it can be handled by neural networks\n",
    "features = np.array(features)\n",
    "# You can ignore that if using rgb. If you want to use grayscale, you should put 1 instead of 3.\n",
    "features = features.reshape(len(features), 128, 128, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalize the image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Because the pixel ratio is between 0 and 255:\n",
    "X = features / 255.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(10137, 128, 128, 3)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Convert gender and age to numpy array:\n",
    "y_gender = np.array(df['gender'])\n",
    "y_age = np.array(df['age'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have an image as an input, and we get back 2 outputs:\n",
    "* Age(Regression problem)\n",
    "* Gender(Classification problem)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Model creation\n",
    "from keras.layers import MaxPooling2D, Flatten, Dense, Dropout, Conv2D\n",
    "from keras import Input, Model\n",
    "\n",
    "inputs = Input(input_shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# convolutional layers:\n",
    "# 32 is the number of filters\n",
    "conv_1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "maxp_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n",
    "conv_2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(maxp_1)\n",
    "maxp_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n",
    "conv_3 = Conv2D(128, kernel_size=(3, 3), activation='relu')(maxp_2)\n",
    "maxp_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n",
    "conv_4 = Conv2D(256, kernel_size=(3, 3), activation='relu')(maxp_3)\n",
    "maxp_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n",
    "\n",
    "# Flat everything: The convulational layer will be 'flattened' to a single dimension.\n",
    "flatten = Flatten()(maxp_4)\n",
    "\n",
    "# Fully connected layers\n",
    "dense_1 = Dense(256, activation='relu')(flatten)\n",
    "dense_2 = Dense(256, activation='relu')(flatten)\n",
    "\n",
    "dropout_1 = Dropout(0.3)(dense_1)\n",
    "dropout_2 = Dropout(0.3)(dense_2)\n",
    "\n",
    "# there is 1 because there is just one output.\n",
    "output_1 = Dense(1, activation='sigmoid', name='gender_out')(dropout_1)\n",
    "output_2 = Dense(1, activation='relu', name='age_out')(dropout_2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs], outputs=[output_1, output_2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "model.compile(loss=['binary_crossentropy', 'mae'], optimizer='adam', metrics=['accuracy'])\n",
    "# binary_crossentropy is for the gender, while mae is for the regression (age)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 126, 126, 32  896         ['input_6[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 63, 63, 32)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 61, 61, 64)   18496       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 30, 30, 64)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 28, 28, 128)  73856       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 128)  0          ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 12, 12, 256)  295168      ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 256)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 9216)         0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          2359552     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          2359552     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " gender_out (Dense)             (None, 1)            257         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " age_out (Dense)                (None, 1)            257         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,108,034\n",
      "Trainable params: 5,108,034\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# This give us the layers we are having"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "254/254 [==============================] - 391s 2s/step - loss: 15.5501 - gender_out_loss: 0.6943 - age_out_loss: 14.8557 - gender_out_accuracy: 0.5575 - age_out_accuracy: 0.1551 - val_loss: 36.1547 - val_gender_out_loss: 0.6622 - val_age_out_loss: 35.4926 - val_gender_out_accuracy: 0.6134 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "254/254 [==============================] - 324s 1s/step - loss: 13.4162 - gender_out_loss: 0.6460 - age_out_loss: 12.7702 - gender_out_accuracy: 0.6201 - age_out_accuracy: 0.1549 - val_loss: 41.6184 - val_gender_out_loss: 0.6692 - val_age_out_loss: 40.9492 - val_gender_out_accuracy: 0.5828 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "254/254 [==============================] - 305s 1s/step - loss: 12.6892 - gender_out_loss: 0.6336 - age_out_loss: 12.0556 - gender_out_accuracy: 0.6319 - age_out_accuracy: 0.1561 - val_loss: 36.9904 - val_gender_out_loss: 0.6424 - val_age_out_loss: 36.3480 - val_gender_out_accuracy: 0.6109 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "254/254 [==============================] - 291s 1s/step - loss: 11.9088 - gender_out_loss: 0.6105 - age_out_loss: 11.2983 - gender_out_accuracy: 0.6585 - age_out_accuracy: 0.1537 - val_loss: 38.5938 - val_gender_out_loss: 0.6610 - val_age_out_loss: 37.9328 - val_gender_out_accuracy: 0.5991 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "254/254 [==============================] - 367s 1s/step - loss: 11.0513 - gender_out_loss: 0.5865 - age_out_loss: 10.4648 - gender_out_accuracy: 0.6822 - age_out_accuracy: 0.1508 - val_loss: 35.5550 - val_gender_out_loss: 0.6474 - val_age_out_loss: 34.9076 - val_gender_out_accuracy: 0.6139 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "254/254 [==============================] - 380s 1s/step - loss: 10.2070 - gender_out_loss: 0.5583 - age_out_loss: 9.6488 - gender_out_accuracy: 0.7032 - age_out_accuracy: 0.1527 - val_loss: 36.5430 - val_gender_out_loss: 0.6055 - val_age_out_loss: 35.9375 - val_gender_out_accuracy: 0.6632 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "254/254 [==============================] - 391s 2s/step - loss: 9.6650 - gender_out_loss: 0.5471 - age_out_loss: 9.1179 - gender_out_accuracy: 0.7085 - age_out_accuracy: 0.1535 - val_loss: 33.4102 - val_gender_out_loss: 0.6123 - val_age_out_loss: 32.7980 - val_gender_out_accuracy: 0.6657 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "254/254 [==============================] - 364s 1s/step - loss: 9.3467 - gender_out_loss: 0.5249 - age_out_loss: 8.8218 - gender_out_accuracy: 0.7252 - age_out_accuracy: 0.1505 - val_loss: 36.5375 - val_gender_out_loss: 0.5995 - val_age_out_loss: 35.9379 - val_gender_out_accuracy: 0.6726 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "254/254 [==============================] - 300s 1s/step - loss: 8.7570 - gender_out_loss: 0.4993 - age_out_loss: 8.2577 - gender_out_accuracy: 0.7487 - age_out_accuracy: 0.1533 - val_loss: 30.9904 - val_gender_out_loss: 0.6024 - val_age_out_loss: 30.3879 - val_gender_out_accuracy: 0.6760 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "254/254 [==============================] - 330s 1s/step - loss: 8.1344 - gender_out_loss: 0.4687 - age_out_loss: 7.6657 - gender_out_accuracy: 0.7707 - age_out_accuracy: 0.1533 - val_loss: 34.9600 - val_gender_out_loss: 0.6159 - val_age_out_loss: 34.3441 - val_gender_out_accuracy: 0.6815 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "254/254 [==============================] - 308s 1s/step - loss: 7.6985 - gender_out_loss: 0.4462 - age_out_loss: 7.2522 - gender_out_accuracy: 0.7805 - age_out_accuracy: 0.1529 - val_loss: 34.8338 - val_gender_out_loss: 0.5891 - val_age_out_loss: 34.2446 - val_gender_out_accuracy: 0.6859 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "254/254 [==============================] - 265s 1s/step - loss: 7.1696 - gender_out_loss: 0.4156 - age_out_loss: 6.7539 - gender_out_accuracy: 0.7984 - age_out_accuracy: 0.1517 - val_loss: 32.1701 - val_gender_out_loss: 0.6132 - val_age_out_loss: 31.5569 - val_gender_out_accuracy: 0.6839 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "254/254 [==============================] - 243s 957ms/step - loss: 6.6729 - gender_out_loss: 0.3900 - age_out_loss: 6.2829 - gender_out_accuracy: 0.8140 - age_out_accuracy: 0.1525 - val_loss: 30.3453 - val_gender_out_loss: 0.6126 - val_age_out_loss: 29.7327 - val_gender_out_accuracy: 0.6933 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "254/254 [==============================] - 250s 984ms/step - loss: 6.1781 - gender_out_loss: 0.3652 - age_out_loss: 5.8128 - gender_out_accuracy: 0.8327 - age_out_accuracy: 0.1525 - val_loss: 33.1828 - val_gender_out_loss: 0.6210 - val_age_out_loss: 32.5617 - val_gender_out_accuracy: 0.6898 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "254/254 [==============================] - 276s 1s/step - loss: 5.7498 - gender_out_loss: 0.3272 - age_out_loss: 5.4226 - gender_out_accuracy: 0.8555 - age_out_accuracy: 0.1527 - val_loss: 32.3042 - val_gender_out_loss: 0.6137 - val_age_out_loss: 31.6905 - val_gender_out_accuracy: 0.6918 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "254/254 [==============================] - 277s 1s/step - loss: 5.4493 - gender_out_loss: 0.2967 - age_out_loss: 5.1527 - gender_out_accuracy: 0.8641 - age_out_accuracy: 0.1517 - val_loss: 29.9754 - val_gender_out_loss: 0.6780 - val_age_out_loss: 29.2973 - val_gender_out_accuracy: 0.6923 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "254/254 [==============================] - 294s 1s/step - loss: 5.0174 - gender_out_loss: 0.2676 - age_out_loss: 4.7497 - gender_out_accuracy: 0.8859 - age_out_accuracy: 0.1535 - val_loss: 30.5474 - val_gender_out_loss: 0.7337 - val_age_out_loss: 29.8137 - val_gender_out_accuracy: 0.6953 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "254/254 [==============================] - 346s 1s/step - loss: 4.7045 - gender_out_loss: 0.2369 - age_out_loss: 4.4675 - gender_out_accuracy: 0.9037 - age_out_accuracy: 0.1545 - val_loss: 31.5897 - val_gender_out_loss: 0.7140 - val_age_out_loss: 30.8757 - val_gender_out_accuracy: 0.6997 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "254/254 [==============================] - 381s 2s/step - loss: 4.5845 - gender_out_loss: 0.2155 - age_out_loss: 4.3689 - gender_out_accuracy: 0.9091 - age_out_accuracy: 0.1524 - val_loss: 31.2416 - val_gender_out_loss: 0.7234 - val_age_out_loss: 30.5183 - val_gender_out_accuracy: 0.6834 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "254/254 [==============================] - 391s 2s/step - loss: 4.2153 - gender_out_loss: 0.1901 - age_out_loss: 4.0253 - gender_out_accuracy: 0.9195 - age_out_accuracy: 0.1523 - val_loss: 32.1540 - val_gender_out_loss: 0.8613 - val_age_out_loss: 31.2927 - val_gender_out_accuracy: 0.6805 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "254/254 [==============================] - 414s 2s/step - loss: 4.2098 - gender_out_loss: 0.1695 - age_out_loss: 4.0403 - gender_out_accuracy: 0.9339 - age_out_accuracy: 0.1535 - val_loss: 30.3820 - val_gender_out_loss: 0.9293 - val_age_out_loss: 29.4527 - val_gender_out_accuracy: 0.6844 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "254/254 [==============================] - 355s 1s/step - loss: 3.9063 - gender_out_loss: 0.1534 - age_out_loss: 3.7529 - gender_out_accuracy: 0.9403 - age_out_accuracy: 0.1539 - val_loss: 31.1717 - val_gender_out_loss: 0.8111 - val_age_out_loss: 30.3605 - val_gender_out_accuracy: 0.7046 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "254/254 [==============================] - 443s 2s/step - loss: 3.7463 - gender_out_loss: 0.1368 - age_out_loss: 3.6095 - gender_out_accuracy: 0.9497 - age_out_accuracy: 0.1533 - val_loss: 30.5922 - val_gender_out_loss: 0.8644 - val_age_out_loss: 29.7278 - val_gender_out_accuracy: 0.7056 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "254/254 [==============================] - 496s 2s/step - loss: 3.6803 - gender_out_loss: 0.1163 - age_out_loss: 3.5640 - gender_out_accuracy: 0.9556 - age_out_accuracy: 0.1534 - val_loss: 33.8378 - val_gender_out_loss: 0.8690 - val_age_out_loss: 32.9688 - val_gender_out_accuracy: 0.6997 - val_age_out_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "254/254 [==============================] - ETA: 0s - loss: 3.6353 - gender_out_loss: 0.1061 - age_out_loss: 3.5293 - gender_out_accuracy: 0.9628 - age_out_accuracy: 0.1545"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-33-f05e6ecfa9f3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mplot_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0my_gender\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_age\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_split\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;31m# validation_split stablished that 0.2 is for testing and 0.8 pro training.input_shape = (128, 128, 3)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1604\u001B[0m                             \u001B[0msteps_per_execution\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_steps_per_execution\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1605\u001B[0m                         )\n\u001B[1;32m-> 1606\u001B[1;33m                     val_logs = self.evaluate(\n\u001B[0m\u001B[0;32m   1607\u001B[0m                         \u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mval_x\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1608\u001B[0m                         \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mval_y\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mevaluate\u001B[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m   1945\u001B[0m                         ):\n\u001B[0;32m   1946\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_test_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1947\u001B[1;33m                             \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1948\u001B[0m                             \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1949\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 915\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    916\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    917\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    952\u001B[0m       \u001B[1;31m# In this case we have not created variables on the first call. So we can\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    953\u001B[0m       \u001B[1;31m# run the first trace but we should fail if variables are created.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 954\u001B[1;33m       \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    955\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_created_variables\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    956\u001B[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2494\u001B[0m       (graph_function,\n\u001B[0;32m   2495\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[1;33m     return graph_function._call_flat(\n\u001B[0m\u001B[0;32m   2497\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0;32m   2498\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1860\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1861\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1862\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1863\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1864\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    498\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 499\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    500\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    501\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\vio_g\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 54\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     55\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=X, y=[y_gender, y_age], batch_size=32, epochs=30, validation_split=0.2)\n",
    "# validation_split stablished that 0.2 is for testing and 0.8 pro training.input_shape = (128, 128, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot the Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = history.history['gender_out_accuracy']\n",
    "val_acc = history.history['val_gender_out_accuracy']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs, accuracy, 'b', label='Training Accurac')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "loss = history.history['gender_out_loss']\n",
    "val_loss = history.history['val_gender_out_loss']\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction with random Picture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_index = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = model.predict(X[image_index].reshape(1, 128, 128, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_gender = prediction[0][0][0]\n",
    "pred_age = prediction[1][0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Real gender: {df['gender'][image_index]}\")\n",
    "print(f\"Predict gender: {pred_gender}\")\n",
    "print(f\"Real age: {df['age'][image_index]}\")\n",
    "print(f\"Real gender: {pred_age}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
